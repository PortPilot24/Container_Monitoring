# llm_summary.py
from openai import OpenAI
from typing import List
import os
from dotenv import load_dotenv

load_dotenv()

# âœ… í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("âŒ OPENAI_API_KEYê°€ .envì—ì„œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
client = OpenAI(api_key=api_key)

def generate_occupancy_summary(predictions: List[float]) -> str:
    print("ğŸ“¥ ë“¤ì–´ì˜¨ ì˜ˆì¸¡ê°’:", predictions)

    if not predictions:
        return "ì˜ˆì¸¡ëœ ì ìœ ìœ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    try:
        scaled_preds = [round(p * 100, 2) for p in predictions]
        prompt = (
            f"í–¥í›„ 3ì‹œê°„ ë™ì•ˆì˜ ì»¨í…Œì´ë„ˆ ì¥ì¹˜ì¥ ì ìœ ìœ¨ ì˜ˆì¸¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {scaled_preds}%. "
            "ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜¼ì¡ë„ ìƒíƒœë¥¼ ìš”ì•½í•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. "
            "í˜¼ì¡ ì—¬ë¶€ì™€ í•„ìš”í•œ ëŒ€ì‘ ì¡°ì¹˜ë„ í•¨ê»˜ ì–¸ê¸‰í•´ì£¼ì„¸ìš”."
        )

        print("ğŸ“¡ OpenAI API ìš”ì²­ ì¤‘...")

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” í•­ë§Œ ìš´ì˜ ì „ë¬¸ê°€ì•¼."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=300,
            temperature=0.7
        )

        result = response.choices[0].message.content
        print("âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ:", result)
        return result

    except Exception as e:
        print("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨:", e)
        raise RuntimeError(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")